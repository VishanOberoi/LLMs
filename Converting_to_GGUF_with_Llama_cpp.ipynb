{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNswblrs0GMZEo3o/A/IXIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a680b542840f466daec7d4dfd5b0bfc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_823ef946b1d44d298a9171b791ccf0c2",
              "IPY_MODEL_9cbd9a82634e45ae8cb896b8ca22ce6f",
              "IPY_MODEL_bdb80108aa2440a1b3690425bb88ca03"
            ],
            "layout": "IPY_MODEL_d2194a50f79f4ab1b8a60d1c530c8dbb"
          }
        },
        "823ef946b1d44d298a9171b791ccf0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57427f2664ad4a8b8f58285e38ae85fb",
            "placeholder": "​",
            "style": "IPY_MODEL_5b1358f6d38b420488a0cbbed4133869",
            "value": "finetuned-2.gguf: 100%"
          }
        },
        "9cbd9a82634e45ae8cb896b8ca22ce6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8674d1190da49f682a9e70f5d1167a1",
            "max": 7161090592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4109b5c9fa9474086ad908ca8a9ab45",
            "value": 7161090592
          }
        },
        "bdb80108aa2440a1b3690425bb88ca03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e1dd7d57ad741de9c2a8821ce2a109e",
            "placeholder": "​",
            "style": "IPY_MODEL_f83c2069ee1a48da883c6e89cface2c4",
            "value": " 7.16G/7.16G [02:23&lt;00:00, 56.2MB/s]"
          }
        },
        "d2194a50f79f4ab1b8a60d1c530c8dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57427f2664ad4a8b8f58285e38ae85fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1358f6d38b420488a0cbbed4133869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8674d1190da49f682a9e70f5d1167a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4109b5c9fa9474086ad908ca8a9ab45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e1dd7d57ad741de9c2a8821ce2a109e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83c2069ee1a48da883c6e89cface2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishanOberoi/LLMs/blob/main/Converting_to_GGUF_with_Llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversion\n",
        "\n",
        "Use this notebook to convert to GGUF format.\n",
        "\n",
        "About GGUF\n",
        "GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n",
        "\n",
        "Here is an incomplate list of clients and libraries that are known to support GGUF:\n",
        "\n",
        "llama.cpp os source project for GGUF. Offers a CLI and a server option."
      ],
      "metadata": {
        "id": "dyswoJXAdd0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smHu8hADeYjQ",
        "outputId": "5d060564-0c63-46ce-a477-06bd8c08fb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'finetunedmodelpath'...\n",
            "fatal: could not read Username for 'https://huggingface.co': No such device or address\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 18881, done.\u001b[K\n",
            "remote: Counting objects: 100% (6027/6027), done.\u001b[K\n",
            "remote: Compressing objects: 100% (468/468), done.\u001b[K\n",
            "remote: Total 18881 (delta 5826), reused 5564 (delta 5559), pack-reused 12854\u001b[K\n",
            "Receiving objects: 100% (18881/18881), 21.51 MiB | 22.27 MiB/s, done.\n",
            "Resolving deltas: 100% (13273/13273), done.\n"
          ]
        }
      ],
      "source": [
        "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
        "!git lfs install\n",
        "# Clone your model from Huggingface\n",
        "!git clone https://huggingface.co/vishanoberoi/Llama-2-7b-chat-hf-fine-tuned\n",
        "# Clone llama.cpp's repository. They provide code to convert models into gguf.\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/vishanoberoi/Llama-2-7b-chat-hf-fine-tuned\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x1C7a6hgqk8",
        "outputId": "3c88bb40-8808-4d8f-9202-784439bad2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Llama-2-7b-chat-hf-fine-tuned'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)\u001b[K\rremote: Counting objects:  15% (2/13)\u001b[K\rremote: Counting objects:  23% (3/13)\u001b[K\rremote: Counting objects:  30% (4/13)\u001b[K\rremote: Counting objects:  38% (5/13)\u001b[K\rremote: Counting objects:  46% (6/13)\u001b[K\rremote: Counting objects:  53% (7/13)\u001b[K\rremote: Counting objects:  61% (8/13)\u001b[K\rremote: Counting objects:  69% (9/13)\u001b[K\rremote: Counting objects:  76% (10/13)\u001b[K\rremote: Counting objects:  84% (11/13)\u001b[K\rremote: Counting objects:  92% (12/13)\u001b[K\rremote: Counting objects: 100% (13/13)\u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects:   7% (1/13)\u001b[K\rremote: Compressing objects:  15% (2/13)\u001b[K\rremote: Compressing objects:  23% (3/13)\u001b[K\rremote: Compressing objects:  30% (4/13)\u001b[K\rremote: Compressing objects:  38% (5/13)\u001b[K\rremote: Compressing objects:  46% (6/13)\u001b[K\rremote: Compressing objects:  53% (7/13)\u001b[K\rremote: Compressing objects:  61% (8/13)\u001b[K\rremote: Compressing objects:  69% (9/13)\u001b[K\rremote: Compressing objects:  76% (10/13)\u001b[K\rremote: Compressing objects:  84% (11/13)\u001b[K\rremote: Compressing objects:  92% (12/13)\u001b[K\rremote: Compressing objects: 100% (13/13)\u001b[K\rremote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "Unpacking objects:   6% (1/16)\rUnpacking objects:  12% (2/16)\rUnpacking objects:  18% (3/16)\rUnpacking objects:  25% (4/16)\rUnpacking objects:  31% (5/16)\rUnpacking objects:  37% (6/16)\rUnpacking objects:  43% (7/16)\rUnpacking objects:  50% (8/16)\rUnpacking objects:  56% (9/16)\rUnpacking objects:  62% (10/16)\rUnpacking objects:  68% (11/16)\rUnpacking objects:  75% (12/16)\rUnpacking objects:  81% (13/16)\rremote: Total 16 (delta 1), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (16/16), 482.11 KiB | 7.53 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 12.99 MiB/s, done.\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00001-of-00003.safetensors\n",
            "\tmodel-00002-of-00003.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/llama.cpp/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CsKwgzp2ebw6",
        "outputId": "31aa10e8-6710-4adf-8cb3-845e22ec0bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy~=1.24.4 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.35.2)\n",
            "Collecting gguf>=0.1.0 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 torch-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/llama.cpp/convert.py /content/Llama-2-7b-chat-hf-fine-tuned --outfile finetuned-2.gguf --outtype q8_0 #quantize in 8-bit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9xQNpSweeMx",
        "outputId": "4843a7d4-907e-496c-84b6-72903b81b032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00001-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00001-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00002-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00003-of-00003.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned'))\n",
            "Found vocab files: {'tokenizer.model': None, 'vocab.json': None, 'tokenizer.json': PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned/tokenizer.json')}\n",
            "Loading vocab file '/content/Llama-2-7b-chat-hf-fine-tuned/tokenizer.json', type 'spm'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert.py\", line 1483, in <module>\n",
            "    main()\n",
            "  File \"/content/llama.cpp/convert.py\", line 1451, in main\n",
            "    vocab, special_vocab = vocab_factory.load_vocab(args.vocab_type, model_parent_path)\n",
            "  File \"/content/llama.cpp/convert.py\", line 1336, in load_vocab\n",
            "    vocab = SentencePieceVocab(\n",
            "  File \"/content/llama.cpp/convert.py\", line 394, in __init__\n",
            "    self.sentencepiece_tokenizer = SentencePieceProcessor(str(fname_tokenizer))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\", line 447, in Init\n",
            "    self.Load(model_file=model_file, model_proto=model_proto)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\", line 905, in Load\n",
            "    return self.LoadFromFile(model_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\", line 310, in LoadFromFile\n",
            "    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\n",
            "RuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(), serialized.size())] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/llama.cpp/convert.py /content/Llama-2-7b-chat-hf-fine-tuned \\\n",
        "   --vocab-type hfft \\\n",
        "  --outfile /content/finetuned-2.gguf \\\n",
        "  --outtype q8_0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFXJgDNygETE",
        "outputId": "7113ca2c-b73d-494c-ee2c-ed7e6b98c37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00001-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00001-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00002-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00003-of-00003.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned'))\n",
            "Found vocab files: {'tokenizer.model': None, 'vocab.json': None, 'tokenizer.json': PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned/tokenizer.json')}\n",
            "Loading vocab file '/content/Llama-2-7b-chat-hf-fine-tuned', type 'hfft'\n",
            "fname_tokenizer: /content/Llama-2-7b-chat-hf-fine-tuned\n",
            "Vocab info: <HfVocab with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 0}, add special tokens unset>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "skipping tensor blk.0.attn_rot_embd\n",
            "skipping tensor blk.1.attn_rot_embd\n",
            "skipping tensor blk.10.attn_rot_embd\n",
            "skipping tensor blk.11.attn_rot_embd\n",
            "skipping tensor blk.2.attn_rot_embd\n",
            "skipping tensor blk.3.attn_rot_embd\n",
            "skipping tensor blk.4.attn_rot_embd\n",
            "skipping tensor blk.5.attn_rot_embd\n",
            "skipping tensor blk.6.attn_rot_embd\n",
            "skipping tensor blk.7.attn_rot_embd\n",
            "skipping tensor blk.8.attn_rot_embd\n",
            "skipping tensor blk.9.attn_rot_embd\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
            "skipping tensor blk.12.attn_rot_embd\n",
            "skipping tensor blk.13.attn_rot_embd\n",
            "skipping tensor blk.14.attn_rot_embd\n",
            "skipping tensor blk.15.attn_rot_embd\n",
            "skipping tensor blk.16.attn_rot_embd\n",
            "skipping tensor blk.17.attn_rot_embd\n",
            "skipping tensor blk.18.attn_rot_embd\n",
            "skipping tensor blk.19.attn_rot_embd\n",
            "skipping tensor blk.20.attn_rot_embd\n",
            "skipping tensor blk.21.attn_rot_embd\n",
            "skipping tensor blk.22.attn_rot_embd\n",
            "skipping tensor blk.23.attn_rot_embd\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
            "skipping tensor blk.24.attn_rot_embd\n",
            "skipping tensor blk.25.attn_rot_embd\n",
            "skipping tensor blk.26.attn_rot_embd\n",
            "skipping tensor blk.27.attn_rot_embd\n",
            "skipping tensor blk.28.attn_rot_embd\n",
            "skipping tensor blk.29.attn_rot_embd\n",
            "skipping tensor blk.30.attn_rot_embd\n",
            "skipping tensor blk.31.attn_rot_embd\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
            "Writing /content/finetuned-2.gguf, format 7\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 0\n",
            "gguf: Setting chat_template to {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
            "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+  27\n",
            "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  28\n",
            "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  28\n",
            "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  29\n",
            "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  32\n",
            "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  34\n",
            "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  34\n",
            "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  34\n",
            "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  34\n",
            "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  35\n",
            "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  35\n",
            "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  42\n",
            "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  45\n",
            "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  46\n",
            "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  46\n",
            "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  46\n",
            "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  46\n",
            "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  46\n",
            "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  47\n",
            "[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
            "[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  56\n",
            "[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  59\n",
            "[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  60\n",
            "[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
            "[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
            "[ 26/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
            "[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
            "[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
            "[ 29/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  67\n",
            "[ 30/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  67\n",
            "[ 31/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  67\n",
            "[ 32/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  67\n",
            "[ 33/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  68\n",
            "[ 34/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  68\n",
            "[ 35/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  74\n",
            "[ 36/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  78\n",
            "[ 37/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  79\n",
            "[ 38/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  79\n",
            "[ 39/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  79\n",
            "[ 40/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  79\n",
            "[ 41/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  79\n",
            "[ 42/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  79\n",
            "[ 43/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  79\n",
            "[ 44/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  89\n",
            "[ 45/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  93\n",
            "[ 46/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  94\n",
            "[ 47/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  94\n",
            "[ 48/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  94\n",
            "[ 49/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  95\n",
            "[ 50/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  95\n",
            "[ 51/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  95\n",
            "[ 52/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  95\n",
            "[ 53/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 101\n",
            "[ 54/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 106\n",
            "[ 55/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 106\n",
            "[ 56/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+ 106\n",
            "[ 57/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 106\n",
            "[ 58/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 107\n",
            "[ 59/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 107\n",
            "[ 60/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 107\n",
            "[ 61/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+ 107\n",
            "[ 62/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 114\n",
            "[ 63/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 117\n",
            "[ 64/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 120\n",
            "[ 65/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+ 120\n",
            "[ 66/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 120\n",
            "[ 67/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 121\n",
            "[ 68/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 121\n",
            "[ 69/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 121\n",
            "[ 70/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+ 121\n",
            "[ 71/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 125\n",
            "[ 72/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 130\n",
            "[ 73/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 131\n",
            "[ 74/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+ 131\n",
            "[ 75/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 131\n",
            "[ 76/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 131\n",
            "[ 77/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 132\n",
            "[ 78/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 132\n",
            "[ 79/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 132\n",
            "[ 80/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 142\n",
            "[ 81/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 143\n",
            "[ 82/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 145\n",
            "[ 83/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 146\n",
            "[ 84/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 146\n",
            "[ 85/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 146\n",
            "[ 86/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 146\n",
            "[ 87/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 146\n",
            "[ 88/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 146\n",
            "[ 89/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 152\n",
            "[ 90/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 156\n",
            "[ 91/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 156\n",
            "[ 92/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 157\n",
            "[ 93/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 157\n",
            "[ 94/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 157\n",
            "[ 95/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 158\n",
            "[ 96/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 158\n",
            "[ 97/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 158\n",
            "[ 98/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 166\n",
            "[ 99/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 169\n",
            "[100/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 170\n",
            "[101/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 170\n",
            "[102/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 170\n",
            "[103/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 170\n",
            "[104/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 170\n",
            "[105/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 170\n",
            "[106/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+ 170\n",
            "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 179\n",
            "[108/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 181\n",
            "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+ 181\n",
            "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+ 181\n",
            "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 186\n",
            "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 188\n",
            "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 189\n",
            "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+ 189\n",
            "[115/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 189\n",
            "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 189\n",
            "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 189\n",
            "[118/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 189\n",
            "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+ 190\n",
            "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 196\n",
            "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 200\n",
            "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 201\n",
            "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+ 202\n",
            "[124/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 202\n",
            "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 202\n",
            "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 202\n",
            "[127/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 202\n",
            "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+ 202\n",
            "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 214\n",
            "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 219\n",
            "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 220\n",
            "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+ 220\n",
            "[133/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 220\n",
            "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 220\n",
            "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 220\n",
            "[136/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 220\n",
            "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 220\n",
            "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 228\n",
            "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 231\n",
            "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 232\n",
            "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 233\n",
            "[142/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 233\n",
            "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 233\n",
            "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 233\n",
            "[145/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 233\n",
            "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 233\n",
            "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 240\n",
            "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 244\n",
            "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 245\n",
            "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 246\n",
            "[151/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 246\n",
            "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 246\n",
            "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 246\n",
            "[154/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 246\n",
            "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 246\n",
            "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 251\n",
            "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 257\n",
            "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 257\n",
            "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 258\n",
            "[160/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 258\n",
            "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[163/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 259\n",
            "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 267\n",
            "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 270\n",
            "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 271\n",
            "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 272\n",
            "[169/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 272\n",
            "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 272\n",
            "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 272\n",
            "[172/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 272\n",
            "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 272\n",
            "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 278\n",
            "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 282\n",
            "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 283\n",
            "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 284\n",
            "[178/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 284\n",
            "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 284\n",
            "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 284\n",
            "[181/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 284\n",
            "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 284\n",
            "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 293\n",
            "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 295\n",
            "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 296\n",
            "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 296\n",
            "[187/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 296\n",
            "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 296\n",
            "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 296\n",
            "[190/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 296\n",
            "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 296\n",
            "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 305\n",
            "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 309\n",
            "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 311\n",
            "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 312\n",
            "[196/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 312\n",
            "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 312\n",
            "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 312\n",
            "[199/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 312\n",
            "[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 312\n",
            "[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 318\n",
            "[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 325\n",
            "[203/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 326\n",
            "[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 326\n",
            "[205/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 326\n",
            "[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 326\n",
            "[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 326\n",
            "[208/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 326\n",
            "[209/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 328\n",
            "[210/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 334\n",
            "[211/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 334\n",
            "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 334\n",
            "[213/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 334\n",
            "[214/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 334\n",
            "[215/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+ 353\n",
            "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 353\n",
            "[217/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 353\n",
            "[218/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 353\n",
            "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 353\n",
            "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 353\n",
            "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 354\n",
            "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 356\n",
            "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 357\n",
            "[224/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 357\n",
            "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 357\n",
            "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 357\n",
            "[227/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 359\n",
            "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 359\n",
            "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 366\n",
            "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 368\n",
            "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 369\n",
            "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 370\n",
            "[233/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 370\n",
            "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 370\n",
            "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 370\n",
            "[236/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 370\n",
            "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 370\n",
            "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 378\n",
            "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 381\n",
            "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 382\n",
            "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 382\n",
            "[242/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 382\n",
            "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 382\n",
            "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 382\n",
            "[245/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 382\n",
            "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 382\n",
            "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 392\n",
            "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 394\n",
            "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 395\n",
            "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 395\n",
            "[251/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 395\n",
            "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 395\n",
            "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 395\n",
            "[254/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 395\n",
            "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 395\n",
            "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 404\n",
            "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 407\n",
            "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 408\n",
            "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 408\n",
            "[260/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 408\n",
            "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 408\n",
            "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 408\n",
            "[263/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 408\n",
            "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 408\n",
            "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 415\n",
            "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 419\n",
            "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 420\n",
            "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 420\n",
            "[269/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 420\n",
            "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 420\n",
            "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 420\n",
            "[272/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 420\n",
            "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 420\n",
            "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 427\n",
            "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 432\n",
            "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 433\n",
            "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 433\n",
            "[278/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 433\n",
            "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 433\n",
            "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 433\n",
            "[281/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 434\n",
            "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 434\n",
            "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 439\n",
            "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 442\n",
            "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 443\n",
            "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 443\n",
            "[287/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 443\n",
            "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 443\n",
            "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 443\n",
            "[290/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 443\n",
            "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 443\n",
            "Wrote /content/finetuned-2.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass your hf-token as environment variable\n",
        "!export HUGGING_FACE_HUB_TOKEN= HF_TOKEN\n"
      ],
      "metadata": {
        "id": "wb_bj4h0koui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will push the model to HF repository\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"vishanoberoi/Llama-2-7b-chat-hf-finedtuned-to-GGUF\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"finetuned-2.gguf\",\n",
        "    path_in_repo=\"finetuned.gguf\",\n",
        "    repo_id=model_id,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "a680b542840f466daec7d4dfd5b0bfc8",
            "823ef946b1d44d298a9171b791ccf0c2",
            "9cbd9a82634e45ae8cb896b8ca22ce6f",
            "bdb80108aa2440a1b3690425bb88ca03",
            "d2194a50f79f4ab1b8a60d1c530c8dbb",
            "57427f2664ad4a8b8f58285e38ae85fb",
            "5b1358f6d38b420488a0cbbed4133869",
            "c8674d1190da49f682a9e70f5d1167a1",
            "a4109b5c9fa9474086ad908ca8a9ab45",
            "3e1dd7d57ad741de9c2a8821ce2a109e",
            "f83c2069ee1a48da883c6e89cface2c4"
          ]
        },
        "id": "JYY0mz6qm1-j",
        "outputId": "6c80fcb6-11c4-4541-aacb-2171af9eb837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "finetuned-2.gguf:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a680b542840f466daec7d4dfd5b0bfc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/vishanoberoi/Llama-2-7b-chat-hf-finedtuned-to-GGUF/commit/9f471051136148efc3f05f9e6a074ebba3954c0d', commit_message='Upload finetuned.gguf with huggingface_hub', commit_description='', oid='9f471051136148efc3f05f9e6a074ebba3954c0d', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}